{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\win_unicode_console\\__init__.py:31: RuntimeWarning: sys.stdin.encoding == '949', whereas sys.stdout.encoding == 'UTF-8', readline hook consumer may assume they are the same\n",
      "  readline_hook.enable(use_pyreadline=use_pyreadline)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import win_unicode_console\n",
    "win_unicode_console.enable()\n",
    "\n",
    "class HangulLoader:\n",
    "   \n",
    "    def GetImages_Labels(self,  imgpath,labelpath ):\n",
    "        #labels\n",
    "        labels = pd.read_csv( labelpath , sep = ',' ).iloc[:,1]\n",
    "        list_labels = list(labels)\n",
    "        set_labels = list(set(list_labels))\n",
    "        one_hot_vectors = np.eye(len(set_labels), dtype=int)\n",
    "        class_vectors = {}\n",
    "        for i, cls in enumerate(set_labels):\n",
    "            class_vectors[cls] = one_hot_vectors[i]\n",
    "        yList = [ class_vectors[cate_name] for cate_name in list_labels]\n",
    "        yList = np.asarray(yList)\n",
    "        classnum = len(set_labels)\n",
    "        #images \n",
    "        file_list = []\n",
    "        for root,_,fnames in os.walk(imgpath):\n",
    "            fnames.sort(key = (lambda x : int(x.split('_')[1].split('.')[0])))\n",
    "            for filename in fnames:\n",
    "                file_list.append(os.path.join(root, filename))\n",
    "        imglist = [ cv2.imread(path) for path in file_list]\n",
    "\n",
    "        return imglist , yList , classnum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\win_unicode_console\\__init__.py:31: RuntimeWarning: sys.stdin.encoding == '949', whereas sys.stdout.encoding == 'UTF-8', readline hook consumer may assume they are the same\n",
      "  readline_hook.enable(use_pyreadline=use_pyreadline)\n"
     ]
    }
   ],
   "source": [
    "#from Data_Loader import HangulLoader as ld\n",
    "\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras import models , applications , callbacks , optimizers , losses\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import *\n",
    "\n",
    "from Data_Loader import HangulLoader as hanLoader\n",
    "\n",
    "import win_unicode_console\n",
    "win_unicode_console.enable()\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = K.tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "# data x ,y \n",
    "labelspath = r\"data\\hangul-images-data\\labels-map.csv\"\n",
    "dirpath = r\"data\\hangul-images-data\\img\"\n",
    "xs , ys , n_cls = hanLoader().GetImages_Labels( dirpath, labelspath )\n",
    "# load model \n",
    "\n",
    "deepFeature = applications.VGG16(include_top = False , input_shape = (64,64,3) )\n",
    "\n",
    "input = deepFeature.input\n",
    "\n",
    "h = deepFeature.output\n",
    "flat = Flatten()\n",
    "dense1 = Dense( 4096 , activation = \"relu\" )\n",
    "dense2 = Dense( 3096 , activation = \"sigmoid\" )\n",
    "lastLayer = Dense( n_cls , activation = \"sigmoid\"  )\n",
    "\n",
    "classifier = [ h , flat , dense1 , dense2 , lastLayer  ]\n",
    "\n",
    "output = reduce( lambda f,s : s(f) , classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 18800 input samples and 18799 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b4a042c9e1af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mxs\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m             \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1491\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[0;32m   1492\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    218\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 18800 input samples and 18799 target samples."
     ]
    }
   ],
   "source": [
    "model_cnn = models.Model( input , output )\n",
    "\n",
    "model_cnn.compile( loss = losses.categorical_crossentropy , optimizer = optimizers.Adam() , matrics = ['acc'] )\n",
    "\n",
    "\n",
    "hist = model_cnn.fit( xs , ys , batch_size = 32 , epochs  = 10 , validation_split = 0.2 )\n",
    "\n",
    "print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
